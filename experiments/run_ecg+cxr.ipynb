{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies the proposed methods to the ecg data. As data are not publicly available, this can not be directly run, but can easily be adapted to run on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils_generation import *\n",
    "from utils_classification import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/multimodal/missingness_shd_embeddings/'\n",
    "RESULTS_PATH = './results_shd'\n",
    "\n",
    "PLOT_PATH = os.path.join(RESULTS_PATH, 'plots')\n",
    "PRED_PATH = os.path.join(RESULTS_PATH, 'preds')\n",
    "\n",
    "# Define path to open data\n",
    "create_path_and_all_parents(RESULTS_PATH)\n",
    "create_path_and_all_parents(PLOT_PATH)\n",
    "create_path_and_all_parents(PRED_PATH)\n",
    "\n",
    "print(f'Saving results to {RESULTS_PATH}')\n",
    "\n",
    "data_path_dict = {'x1_feature_path':os.path.join(DATA_PATH, 'cxr.csv'),\n",
    "                  'x2_feature_path':os.path.join(DATA_PATH, 'ecg.csv'),\n",
    "                  'demo_feature_path':os.path.join(DATA_PATH, 'demo.csv'),\n",
    "                  'classification_label_path':os.path.join(DATA_PATH, 'labels.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data (each file contains a different modality - columns are features, rows are patients)\n",
    "x1_df = pd.read_csv(data_path_dict['x1_feature_path'], index_col=0)\n",
    "x2_df = pd.read_csv(data_path_dict['x2_feature_path'], index_col=0)\n",
    "\n",
    "# We use an additional demographic feature set to improve ipw estimation\n",
    "demo_df = pd.read_csv(data_path_dict['demo_feature_path'], index_col=0).loc[x2_df.index]\n",
    "\n",
    "\n",
    "# Standardize all data\n",
    "x1_df = pd.DataFrame(StandardScaler().fit_transform(x1_df), \n",
    "                     columns=x1_df.columns, index=x1_df.index)\n",
    "x2_df = pd.DataFrame(StandardScaler().fit_transform(x2_df), \n",
    "                     columns=x2_df.columns, index=x2_df.index)\n",
    "demo_df = pd.DataFrame(StandardScaler().fit_transform(demo_df), \n",
    "                     columns=demo_df.columns, index=demo_df.index)\n",
    "\n",
    "x1_x2_df = pd.concat([x1_df, x2_df], axis = 1).dropna()\n",
    "\n",
    "label_df = pd.read_csv(data_path_dict['classification_label_path'], index_col=0)\n",
    "miss_label_df = label_df.cxr_observed_label == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label file contains the split \n",
    "train_index = label_df.loc[label_df.loc[:, 'missingness_data_split'] == 'train'].index\n",
    "val_index = label_df.loc[label_df.loc[:, 'missingness_data_split'] == 'val'].index\n",
    "test_index = label_df.loc[label_df.loc[:, 'missingness_data_split'] == 'test'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'layers': [[32] * 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate IPW\n",
    "path = os.path.join(PRED_PATH, 'p_m_hat.pickle')\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    p_m_hat = pickle.load(open(path, 'rb'))\n",
    "else:\n",
    "    # Estimate the missingness probabilities - ASSUMING MAR\n",
    "    regressor = pd.concat([demo_df, x2_df], axis = 1)\n",
    "    p_m_hat = train_logistic_regression_and_get_prediction_probabilities(regressor.loc[train_index], \n",
    "                                                        miss_label_df.loc[train_index], \n",
    "                                                        regressor.loc[val_index], \n",
    "                                                        miss_label_df.loc[val_index], \n",
    "                                                        regressor, clip = True)\n",
    "\n",
    "    pickle.dump(p_m_hat, open(path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predictions = os.path.join(PRED_PATH, 'all_setting_predictions.pickle')\n",
    "path_metrics = os.path.join(PRED_PATH, 'all_setting_metrics.pickle')\n",
    "\n",
    "if os.path.isfile(path_predictions):\n",
    "    all_setting_predictions = pickle.load(open(path_predictions, 'rb'))\n",
    "    all_setting_metrics = pickle.load(open(path_metrics, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_predictions = {}\n",
    "    all_setting_metrics = {}\n",
    "\n",
    "for observation in ['corrected', 'observed'] :\n",
    "    for modality, modality_name in zip([x1_df, x2_df, x1_x2_df], ['x1', 'x2', 'x1_x2']):        \n",
    "        if (modality_name, observation) in all_setting_predictions: continue\n",
    "        \n",
    "        all_setting_predictions[(modality_name, observation)] = {}\n",
    "        all_setting_metrics[(modality_name, observation)] = {}\n",
    "\n",
    "        # Compute under missingness \n",
    "        observed = miss_label_df == 0\n",
    "        data = modality.loc[observed]\n",
    "\n",
    "        # Split data\n",
    "        train = data.loc[label_df.loc[observed, 'missingness_data_split'] == 'train']\n",
    "        val = data.loc[label_df.loc[observed, 'missingness_data_split'] == 'val']\n",
    "        test = data.loc[label_df.loc[observed, 'missingness_data_split'] == 'test']\n",
    "        eval = test\n",
    "        \n",
    "\n",
    "        # Estimate IPW weights\n",
    "        p_m = miss_label_df.mean() # observed\n",
    "        p_hat = p_m_hat\n",
    "        ipw_weights = (1 - p_m) / (1 - p_hat)  \n",
    "\n",
    "        # Train with IPW\n",
    "        all_setting_predictions[(modality_name, observation)] = train_mlp_and_get_prediction_probabilities(train, label_df.shd_composite_label.loc[train.index], \n",
    "                                                                                                            val, label_df.shd_composite_label.loc[val.index], \n",
    "                                                                                                            modality, \n",
    "                                                                                                            sample_weight=ipw_weights.loc[train.index] if observation == 'corrected' else None, \n",
    "                                                                                                            weight_val=ipw_weights.loc[val.index] if observation == 'corrected' else None, \n",
    "                                                                                                            grid_search=grid_search)\n",
    "\n",
    "        # Evaluate\n",
    "        all_setting_metrics[(modality_name, observation)] = get_classification_metric_dict(y_true= label_df.shd_composite_label.loc[eval.index], \n",
    "                                                                                        y_pred = all_setting_predictions[(modality_name, observation)].loc[eval.index],\n",
    "                                                                                        ipw_weights= ipw_weights.loc[eval.index] if observation == 'corrected' else None)\n",
    "    \n",
    "        pickle.dump(all_setting_predictions, open(path_predictions, 'wb'))\n",
    "        pickle.dump(all_setting_metrics, open(path_metrics, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for model in all_setting_metrics:\n",
    "    print(model[0].replace('x1', \"CXR\").replace('x2', \"ECG\").replace('_', ' + '), model[1], ' - AUC = {:.2f} ({:.2f})'.format(all_setting_metrics[model]['auroc'], all_setting_metrics[model]['auroc_std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from information_decomposition import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_decomposition = os.path.join(PRED_PATH, 'all_setting_decompositions.pickle')\n",
    "\n",
    "if os.path.isfile(path_decomposition):\n",
    "    all_setting_decompositions = pickle.load(open(path_decomposition, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_decompositions = {}\n",
    "\n",
    "    # Compute under missingness \n",
    "    observed = miss_label_df == 0\n",
    "\n",
    "    # Split data\n",
    "    train = label_df.loc[observed, 'missingness_data_split'] == 'train'\n",
    "    val = label_df.loc[observed, 'missingness_data_split'] == 'val'\n",
    "    test = label_df.loc[observed, 'missingness_data_split'] == 'test'\n",
    "    train, val, test = train[train], val[val], test[test]\n",
    "\n",
    "    p_m = miss_label_df.mean() # observed\n",
    "    p_hat = p_m_hat\n",
    "    ipw_weights = (1 - p_m) / (1 - p_hat)  \n",
    "\n",
    "    # Estimate with IPW weights\n",
    "    p_y_given_x1_x2 = all_setting_predictions[('x1_x2', 'corrected')]\n",
    "    p_y_given_x1 = all_setting_predictions[('x1', 'corrected')]\n",
    "    p_y_given_x2 = all_setting_predictions[('x2', 'corrected')]\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train.index].values, x2_df.loc[train.index].values, \n",
    "                                x1_df.loc[val.index].values, x2_df.loc[val.index].values, \n",
    "                                p_y_given_x1.loc[train.index].values, p_y_given_x2.loc[train.index].values, \n",
    "                                p_y_given_x1.loc[val.index].values, p_y_given_x2.loc[val.index].values,\n",
    "                                ipw_weights.loc[train.index].values, ipw_weights.loc[val.index].values,\n",
    "                                grid_search=grid_search, epochs = 100, device='cuda:0')\n",
    "\n",
    "    all_setting_decompositions['corrected'] = pid_decomposition_batched(estimator, x1_df.loc[test.index].values, x2_df.loc[test.index].values, \n",
    "                                                                        p_y_given_x1.loc[test.index].values, p_y_given_x2.loc[test.index].values,\n",
    "                                                                        p_y_given_x1_x2.loc[test.index].values, \n",
    "                                                                        label_df.shd_composite_label.loc[test.index].values, ipw_weights.loc[test.index].values)\n",
    "\n",
    "    # Compute with no correction\n",
    "    p_y_given_x1_x2 = all_setting_predictions[('x1_x2', 'observed')]\n",
    "    p_y_given_x1 = all_setting_predictions[('x1', 'observed')]\n",
    "    p_y_given_x2 = all_setting_predictions[('x2', 'observed')]\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train.index].values, x2_df.loc[train.index].values,\n",
    "                    x1_df.loc[val.index].values, x2_df.loc[val.index].values,\n",
    "                    p_y_given_x1.loc[train.index].values, p_y_given_x2.loc[train.index].values, \n",
    "                    p_y_given_x1.loc[val.index].values, p_y_given_x2.loc[val.index].values,\n",
    "                    grid_search=grid_search, epochs = 100, device='cuda:0')\n",
    "\n",
    "    all_setting_decompositions['observed'] = pid_decomposition_batched(estimator, x1_df.loc[test.index].values, x2_df.loc[test.index].values, \n",
    "                                                                        p_y_given_x1.loc[test.index].values, p_y_given_x2.loc[test.index].values, \n",
    "                                                                        p_y_given_x1_x2.loc[test.index].values, \n",
    "                                                                        label_df.shd_composite_label.loc[test.index].values)\n",
    "    \n",
    "    pickle.dump(all_setting_decompositions, open(path_decomposition, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PID results\n",
    "for model in all_setting_decompositions:\n",
    "    print(model)\n",
    "    for pid in all_setting_decompositions[model]:\n",
    "        if '_std' in pid: continue\n",
    "        print(pid.replace('1', \"CXR\").replace('2', \"ECG\"), ' = {:.2f} ({:.2f})'.format(all_setting_decompositions[model][pid], all_setting_decompositions[model][pid + '_std']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
