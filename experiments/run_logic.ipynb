{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to recover the motivation examples with binary logit outcomes. In this example, we show how a 50% missingness biases the estimation of performance and PID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils_generation import *\n",
    "from utils_classification import *\n",
    "\n",
    "from information_decomposition import *\n",
    "from information_decomposition.utils import *\n",
    "import torch\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def split(data, label):\n",
    "    \"\"\"\n",
    "    Splits the data into train, val, test and all sets based on the label dataframe.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The data to be split.\n",
    "        label (pd.DataFrame): The labels indicating the split.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the split data.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'train': data.loc[label.loc[:, 'data_split'] == 'train'],\n",
    "        'val': data.loc[label.loc[:, 'data_split'] == 'val'],\n",
    "        'test': data.loc[label.loc[:, 'data_split'] == 'test'],\n",
    "        'all': data\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_feature_dict_from_dataframes(x1_df, x2_df, label_df, standard_scaling = True):\n",
    "    \"\"\"\n",
    "    Creates dictionary with normalised data splitted based on the label dataframe.\n",
    "    Args:\n",
    "        x1_df (pd.DataFrame): The first modality data.\n",
    "        x2_df (pd.DataFrame): The second modality data.\n",
    "        label_df (pd.DataFrame): The labels indicating the split.\n",
    "        standard_scaling (bool): Whether to apply standard scaling to the data.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the split data for each modality.\n",
    "    \"\"\"\n",
    "    x1_x2_df = pd.concat([x1_df, x2_df], axis = 1)\n",
    "    feature_dict = {}\n",
    "    for modality_name, modality in [('x1', x1_df), ('x2', x2_df), ('x1_x2', x1_x2_df)] :\n",
    "        modality_feature_dict = split(modality, label_df)\n",
    "        if standard_scaling :\n",
    "            scaler = StandardScaler()\n",
    "            modality_feature_dict['train'] = pd.DataFrame(scaler.fit_transform(modality_feature_dict['train']), index=modality_feature_dict['train'].index, columns=modality_feature_dict['train'].columns)\n",
    "            modality_feature_dict['val'] = pd.DataFrame(scaler.transform(modality_feature_dict['val']), index=modality_feature_dict['val'].index, columns=modality_feature_dict['val'].columns)\n",
    "            modality_feature_dict['test'] = pd.DataFrame(scaler.transform(modality_feature_dict['test']), index=modality_feature_dict['test'].index, columns=modality_feature_dict['test'].columns)\n",
    "            modality_feature_dict['all'] = pd.DataFrame(scaler.transform(modality_feature_dict['all']), index=modality_feature_dict['all'].index, columns=modality_feature_dict['all'].columns)\n",
    "\n",
    "        feature_dict[modality_name] = modality_feature_dict\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def get_prediction_dict_from_dataframes(feature_dict, label_dict, label_df, sample_weight = None, grid_search = {}) :\n",
    "    \"\"\"\n",
    "    Trains MLP classifiers on the provided data and returns the prediction probabilities.\n",
    "    Args:\n",
    "        feature_dict (dict): A dictionary containing the feature data for each modality.\n",
    "        label_dict (dict): A dictionary containing the labels for each split.\n",
    "        label_df (pd.DataFrame): The labels indicating the split.\n",
    "        sample_weight (dict): A dictionary containing sample weights for each split.\n",
    "        grid_search (dict): A dictionary containing grid search parameters.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the prediction probabilities for each modality.\n",
    "    \"\"\"\n",
    "    prediction_dict = {}\n",
    "    for modality_name in ['x1', 'x2', 'x1_x2'] :\n",
    "        prediction_dict[modality_name] = train_mlp_and_get_prediction_probabilities(X_train = feature_dict[modality_name]['train'],\n",
    "                                                                                    y_train = label_dict['train'],\n",
    "                                                                                    X_val = feature_dict[modality_name]['val'],\n",
    "                                                                                    y_val = label_dict['val'],\n",
    "                                                                                    X = feature_dict[modality_name]['all'],\n",
    "                                                                                    sample_weight = sample_weight['train'] if sample_weight is not None else None,\n",
    "                                                                                    weight_val = sample_weight['val'] if sample_weight is not None else None,\n",
    "                                                                                    grid_search = grid_search)\n",
    "        prediction_dict[modality_name] = split(prediction_dict[modality_name], label_df)\n",
    "\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10000 # Number of samples\n",
    "INFORMATION_SETTING = 'and' # Information setting for the experiment 'and' for AND, 'or' for OR, 'xor' for XOR\n",
    "\n",
    "MISSINGNESS = True # Whether to apply missingness to the data\n",
    "CORRECTION = False # Whether to apply correction to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "x1_df = pd.DataFrame({'x1': np.random.rand(N_SAMPLES) > 0.5})\n",
    "x2_df = pd.DataFrame({'x2': np.random.rand(N_SAMPLES) > 0.5})\n",
    "label_df = pd.DataFrame({'data_split': np.random.choice(['train', 'val', 'test'], N_SAMPLES, p=[0.8, 0.1, 0.1])})\n",
    "label_df['xor'] = ((x1_df.values | x2_df.values) & ~(x1_df.values & x2_df.values)).astype(int)\n",
    "label_df['or'] = ((x1_df.values | x2_df.values)).astype(int)\n",
    "label_df['and'] = ((x1_df.values & x2_df.values)).astype(int)\n",
    "\n",
    "# Generate missingness patterns based on the first modality\n",
    "if MISSINGNESS:\n",
    "    proba_obs = (x1_df['x1'] * 0.6 + 0.2)\n",
    "    observed = pd.Series(np.random.binomial(1, proba_obs.values) == 1, index = x1_df.index)\n",
    "\n",
    "    # Estimate the inverse probability weights\n",
    "    ipw = proba_obs.mean() / proba_obs\n",
    "    ipw_dict = split(ipw[observed], label_df[observed])\n",
    "else:\n",
    "    observed = pd.Series(True, index = x1_df.index) \n",
    "\n",
    "# Split observed data\n",
    "feature_dict = get_feature_dict_from_dataframes(x1_df[observed], x2_df[observed], label_df[observed], standard_scaling = True)\n",
    "label_dict = split(label_df[observed][INFORMATION_SETTING], label_df[observed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the prediction given each modalities\n",
    "grid_search = {'layers': [[32] * 2]}\n",
    "prediction_dict = get_prediction_dict_from_dataframes(feature_dict, label_dict, label_df[observed], \n",
    "                                                      sample_weight = ipw_dict if CORRECTION else None, grid_search=grid_search) \n",
    "\n",
    "# Measure perfrormance\n",
    "y_test = label_dict['test']\n",
    "for modality_name, modality_predictions in prediction_dict.items() :\n",
    "    y_pred = modality_predictions['test']\n",
    "    auroc = []\n",
    "    for bootstrap in range(100) :\n",
    "        sample = np.random.choice(y_test.index, size = len(y_test), replace = True)\n",
    "        auroc.append(roc_auc_score(y_true = y_test.loc[sample], y_score = y_pred.loc[sample], \n",
    "                                   sample_weight = ipw_dict['test'].loc[sample] if CORRECTION else None))\n",
    "    print(f'AUC {modality_name:<10}: {np.mean(auroc):.2f} ({np.std(auroc):.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Q\n",
    "estimator = QEstimator(x1_train = feature_dict['x1']['train'].values,\n",
    "                                 x2_train = feature_dict['x2']['train'].values,\n",
    "                                 x1_val = feature_dict['x1']['val'].values,\n",
    "                                 x2_val = feature_dict['x2']['val'].values,\n",
    "                                 p_y_given_x1_train= prediction_dict['x1']['train'].values,\n",
    "                                 p_y_given_x2_train= prediction_dict['x2']['train'].values,\n",
    "                                 p_y_given_x1_val = prediction_dict['x1']['val'].values,\n",
    "                                 p_y_given_x2_val = prediction_dict['x2']['val'].values,\n",
    "                                 ipw_train= ipw_dict['train'].values if CORRECTION else None,\n",
    "                                 ipw_val= ipw_dict['val'].values if CORRECTION else None,\n",
    "                                 epochs = 100,\n",
    "                                 device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                 grid_search=grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PID\n",
    "pid = pid_decomposition_batched(estimator, feature_dict['x1']['test'].values, feature_dict['x2']['test'].values,\n",
    "                        prediction_dict['x1']['test'].values, prediction_dict['x2']['test'].values,\n",
    "                        prediction_dict['x1_x2']['test'].values, label_dict['test'].values,\n",
    "                        ipw_dict['test'].values if CORRECTION else None)\n",
    "for p in pid:\n",
    "    if 'std' in p:\n",
    "        continue\n",
    "    print(f'PID {p:<10}: {pid[p]:.2f} ({pid[p + \"_std\"]:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
