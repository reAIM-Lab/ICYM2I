{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook executes the proposed information estimation methods on the synthetic data obtained by running the `generate_simulation_data.py` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils_generation import *\n",
    "from utils_classification import *\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False, \"axes.spines.left\": False,\n",
    "                 \"axes.spines.bottom\": False, \"figure.dpi\": 300, 'savefig.dpi': 300}\n",
    "sns.set_theme(style = \"whitegrid\", rc = custom_params, font_scale = 1.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should match the parameters used in the data generation\n",
    "N_SAMPLES = 10000\n",
    "CLASSIFICATION_FLIP_PROB = 0.2\n",
    "DATA_PARENT_PATH = './data'\n",
    "RESULTS_PARENT_PATH = './results'\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_PARENT_PATH, f'nsamples_{N_SAMPLES}_flipprob_{CLASSIFICATION_FLIP_PROB}')\n",
    "RESULTS_PATH = os.path.join(RESULTS_PARENT_PATH, f'nsamples_{N_SAMPLES}_flipprob_{CLASSIFICATION_FLIP_PROB}')\n",
    "PLOT_PATH = os.path.join(RESULTS_PATH, 'plots')\n",
    "PRED_PATH = os.path.join(RESULTS_PATH, 'preds')\n",
    "\n",
    "\n",
    "# Define path to open data\n",
    "create_path_and_all_parents(RESULTS_PATH)\n",
    "create_path_and_all_parents(PLOT_PATH)\n",
    "create_path_and_all_parents(PRED_PATH)\n",
    "\n",
    "print(f'Saving results to {RESULTS_PATH}')\n",
    "\n",
    "data_path_dict = {'x1_feature_path':os.path.join(DATA_PATH, 'continuous_x1_features.csv'),\n",
    "                  'x2_feature_path':os.path.join(DATA_PATH, 'continuous_x2_features.csv'),\n",
    "                  'classification_label_path':os.path.join(DATA_PATH, 'classification_labels.csv'),\n",
    "                  'miss_probs_path':os.path.join(DATA_PATH, 'miss_probs.csv'),\n",
    "                  'miss_label_path':os.path.join(DATA_PATH, 'miss_labels.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV open\n",
    "x1_df = pd.read_csv(data_path_dict['x1_feature_path'])\n",
    "x2_df = pd.read_csv(data_path_dict['x2_feature_path'])\n",
    "x1_x2_df = pd.concat([x1_df, x2_df], axis = 1)\n",
    "\n",
    "label_df = pd.read_csv(data_path_dict['classification_label_path'])\n",
    "miss_label_df = pd.read_csv(data_path_dict['miss_label_path'], header=[0,1])\n",
    "miss_probs_df = pd.read_csv(data_path_dict['miss_probs_path'], header=[0,1])\n",
    "\n",
    "classification_settings =  label_df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'layers': [[32] * 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(PRED_PATH, 'p_m_hat.pickle')\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    p_m_hat = pickle.load(open(path, 'rb'))\n",
    "else:\n",
    "    # Estimate the missingness probabilities - ASSUMING MAR\n",
    "    p_m_hat = {}\n",
    "    for classification_setting in classification_settings:\n",
    "        p_m_hat[classification_setting] = miss_probs_df[(classification_setting, '0.5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = label_df.loc[label_df.loc[:, 'data_split'] == 'train'].index\n",
    "val_index = label_df.loc[label_df.loc[:, 'data_split'] == 'val'].index\n",
    "test_index = label_df.loc[label_df.loc[:, 'data_split'] == 'test'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predictions = os.path.join(PRED_PATH, 'all_setting_predictions.pickle')\n",
    "path_metrics = os.path.join(PRED_PATH, 'all_setting_metrics.pickle')\n",
    "\n",
    "if os.path.isfile(path_predictions):\n",
    "    all_setting_predictions = pickle.load(open(path_predictions, 'rb'))\n",
    "    all_setting_metrics = pickle.load(open(path_metrics, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_predictions = {}\n",
    "    all_setting_metrics = {}\n",
    "\n",
    "# Loop across settings\n",
    "for classification_setting in classification_settings:\n",
    "    print(f'Classification setting: {classification_setting}')\n",
    "\n",
    "    if classification_setting not in all_setting_predictions: \n",
    "        all_setting_predictions[classification_setting] = {}\n",
    "        all_setting_metrics[classification_setting] = {}\n",
    "    \n",
    "    for modality, modality_name in zip([x1_df, x2_df, x1_x2_df], ['x1', 'x2', 'x1_x2']):   \n",
    "        if modality_name in all_setting_predictions[classification_setting]: continue\n",
    "        all_setting_predictions[classification_setting][modality_name] = {}\n",
    "        all_setting_metrics[classification_setting][modality_name] = {}\n",
    "\n",
    "        observed = miss_label_df[(classification_setting, '0.5')] == 0\n",
    "        train_index_class = train_index[observed.loc[train_index]]\n",
    "        val_index_class = val_index[observed.loc[val_index]]\n",
    "        test_index_class = test_index[observed.loc[test_index]]\n",
    "\n",
    "        # Train\n",
    "        all_setting_predictions[classification_setting][modality_name]['all'] = train_mlp_and_get_prediction_probabilities(modality.loc[train_index], label_df.loc[train_index, classification_setting], \n",
    "                                                                                                                    modality.loc[val_index], label_df.loc[val_index, classification_setting], \n",
    "                                                                                                                    modality, grid_search=grid_search)\n",
    "\n",
    "        # Evaluate    \n",
    "        all_setting_metrics[classification_setting][modality_name]['all'] = {'all': get_classification_metric_dict(y_true= label_df.loc[test_index, classification_setting], \n",
    "                                                                                                    y_pred = all_setting_predictions[classification_setting][modality_name]['all'].loc[test_index])}\n",
    "\n",
    "\n",
    "        # Estimate IPW weights\n",
    "        p_hat = p_m_hat[classification_setting]\n",
    "        ipw_weights = 0.5 / (1 - p_hat)  \n",
    "\n",
    "        # Train with normalization\n",
    "        all_setting_predictions[classification_setting][modality_name]['observed'] = train_mlp_and_get_prediction_probabilities(modality.loc[train_index_class], label_df.loc[train_index_class, classification_setting], \n",
    "                                                                                                                    modality.loc[val_index_class], label_df.loc[val_index_class, classification_setting], \n",
    "                                                                                                                    modality, grid_search=grid_search)\n",
    "\n",
    "        # Evaluate    \n",
    "        all_setting_metrics[classification_setting][modality_name]['observed'] = {'all': get_classification_metric_dict(y_true= label_df.loc[test_index, classification_setting], \n",
    "                                                                                                    y_pred = all_setting_predictions[classification_setting][modality_name]['observed'].loc[test_index]),\n",
    "                                                                                  'observed': get_classification_metric_dict(y_true= label_df.loc[test_index_class, classification_setting],\n",
    "                                                                                                    y_pred = all_setting_predictions[classification_setting][modality_name]['observed'].loc[test_index_class]), \n",
    "                                                                                  'corrected': get_classification_metric_dict(y_true= label_df.loc[test_index_class, classification_setting],\n",
    "                                                                                                                              y_pred = all_setting_predictions[classification_setting][modality_name]['observed'].loc[test_index_class],\n",
    "                                                                                                                              ipw_weights=ipw_weights.loc[test_index_class])}\n",
    "\n",
    "        # Train with IPW\n",
    "        all_setting_predictions[classification_setting][modality_name]['corrected'] = train_mlp_and_get_prediction_probabilities(modality.loc[train_index_class], label_df.loc[train_index_class, classification_setting], \n",
    "                                                                                                                    modality.loc[val_index_class], label_df.loc[val_index_class, classification_setting], \n",
    "                                                                                                                    modality, grid_search=grid_search,\n",
    "                                                                                                                    sample_weight=ipw_weights.loc[train_index_class], \n",
    "                                                                                                                    weight_val=ipw_weights.loc[val_index_class])\n",
    "\n",
    "        # Evaluate\n",
    "        all_setting_metrics[classification_setting][modality_name]['corrected'] = {'all': get_classification_metric_dict(y_true= label_df.loc[test_index, classification_setting], \n",
    "                                                                                                    y_pred = all_setting_predictions[classification_setting][modality_name]['corrected'].loc[test_index]),\n",
    "                                                                                  'observed': get_classification_metric_dict(y_true= label_df.loc[test_index_class, classification_setting],\n",
    "                                                                                                    y_pred = all_setting_predictions[classification_setting][modality_name]['corrected'].loc[test_index_class]), \n",
    "                                                                                  'corrected': get_classification_metric_dict(y_true= label_df.loc[test_index_class, classification_setting],\n",
    "                                                                                                                              y_pred = all_setting_predictions[classification_setting][modality_name]['corrected'].loc[test_index_class],\n",
    "                                                                                                                              ipw_weights=ipw_weights.loc[test_index_class])}\n",
    "\n",
    "        # Save predictions and metrics\n",
    "        pickle.dump(all_setting_predictions, open(path_predictions, 'wb'))\n",
    "        pickle.dump(all_setting_metrics, open(path_metrics, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from information_decomposition import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_decomposition = os.path.join(PRED_PATH, 'all_setting_decompositions.pickle')\n",
    "\n",
    "if os.path.isfile(path_decomposition):\n",
    "    all_setting_decompositions = pickle.load(open(path_decomposition, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_decompositions = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "# Loop across settings\n",
    "for classification_setting in np.random.choice(classification_settings, replace = False, size = len(classification_settings)):\n",
    "    print(f'Classification setting: {classification_setting}')\n",
    "    if classification_setting not in all_setting_decompositions:\n",
    "        all_setting_decompositions[classification_setting] = {}\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    p_y_given_x1_x2 = all_setting_predictions[classification_setting]['x1_x2']['all']\n",
    "    p_y_given_x1 = all_setting_predictions[classification_setting]['x1']['all']\n",
    "    p_y_given_x2 = all_setting_predictions[classification_setting]['x2']['all']\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train_index].values, x2_df.loc[train_index].values,\n",
    "                                x1_df.loc[val_index].values, x2_df.loc[val_index].values,\n",
    "                                p_y_given_x1.loc[train_index].values, p_y_given_x2.loc[train_index].values, \n",
    "                                p_y_given_x1.loc[val_index].values, p_y_given_x2.loc[val_index].values,\n",
    "                                grid_search=grid_search, epochs = 100, device='cuda:0')\n",
    "\n",
    "    all_setting_decompositions[classification_setting]['all'] = {'all': pid_decomposition_batched(estimator, x1_df.loc[test_index].values, x2_df.loc[test_index].values, \n",
    "                                            p_y_given_x1.loc[test_index].values, p_y_given_x2.loc[test_index].values, \n",
    "                                            p_y_given_x1_x2.loc[test_index].values, \n",
    "                                            label_df[classification_setting].loc[test_index].values)}\n",
    "\n",
    "    p_hat = p_m_hat[classification_setting]\n",
    "    ipw_weights = 0.5 / (1 - p_hat)  \n",
    "\n",
    "    observed = miss_label_df[(classification_setting, '0.5')] == 0\n",
    "    train_index_class = train_index[observed.loc[train_index]]\n",
    "    val_index_class = val_index[observed.loc[val_index]]\n",
    "    test_index_class = test_index[observed.loc[test_index]]\n",
    "\n",
    "    p_y_given_x1_x2 = all_setting_predictions[classification_setting]['x1_x2']['observed']\n",
    "    p_y_given_x1 = all_setting_predictions[classification_setting]['x1']['observed']\n",
    "    p_y_given_x2 = all_setting_predictions[classification_setting]['x2']['observed']\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train_index_class].values, x2_df.loc[train_index_class].values,\n",
    "                                x1_df.loc[val_index_class].values, x2_df.loc[val_index_class].values,\n",
    "                                p_y_given_x1.loc[train_index_class].values, p_y_given_x2.loc[train_index_class].values, \n",
    "                                p_y_given_x1.loc[val_index_class].values, p_y_given_x2.loc[val_index_class].values,\n",
    "                                grid_search=grid_search, epochs = 100, device='cuda:0')\n",
    "\n",
    "    all_setting_decompositions[classification_setting]['observed'] = {'observed': pid_decomposition_batched(estimator, x1_df.loc[test_index_class].values, x2_df.loc[test_index_class].values, \n",
    "                                            p_y_given_x1.loc[test_index_class].values, p_y_given_x2.loc[test_index_class].values, \n",
    "                                            p_y_given_x1_x2.loc[test_index_class].values, \n",
    "                                            label_df[classification_setting].loc[test_index_class].values),\n",
    "                                        'corrected': pid_decomposition_batched(estimator, x1_df.loc[test_index_class].values, x2_df.loc[test_index_class].values,\n",
    "                                            p_y_given_x1.loc[test_index_class].values, p_y_given_x2.loc[test_index_class].values,\n",
    "                                            p_y_given_x1_x2.loc[test_index_class].values,\n",
    "                                            label_df[classification_setting].loc[test_index_class].values,\n",
    "                                            ipw_weights.loc[test_index_class].values)}\n",
    "    \n",
    "    # Estimate with IPW weights\n",
    "    p_y_given_x1_x2 = all_setting_predictions[classification_setting]['x1_x2']['corrected']\n",
    "    p_y_given_x1 = all_setting_predictions[classification_setting]['x1']['corrected']\n",
    "    p_y_given_x2 = all_setting_predictions[classification_setting]['x2']['corrected']\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train_index_class].values, x2_df.loc[train_index_class].values,\n",
    "                                x1_df.loc[val_index_class].values, x2_df.loc[val_index_class].values,\n",
    "                                p_y_given_x1.loc[train_index_class].values, p_y_given_x2.loc[train_index_class].values, \n",
    "                                p_y_given_x1.loc[val_index_class].values, p_y_given_x2.loc[val_index_class].values,\n",
    "                                ipw_weights.loc[train_index_class].values, ipw_weights.loc[val_index_class].values,\n",
    "                                grid_search=grid_search, epochs = 100, device='cuda:0')\n",
    "\n",
    "    all_setting_decompositions[classification_setting]['corrected'] = {'observed': pid_decomposition_batched(estimator, x1_df.loc[test_index_class].values, x2_df.loc[test_index_class].values, \n",
    "                                            p_y_given_x1.loc[test_index_class].values, p_y_given_x2.loc[test_index_class].values, \n",
    "                                            p_y_given_x1_x2.loc[test_index_class].values, \n",
    "                                            label_df[classification_setting].loc[test_index_class].values),\n",
    "                                        'corrected': pid_decomposition_batched(estimator, x1_df.loc[test_index_class].values, x2_df.loc[test_index_class].values,\n",
    "                                            p_y_given_x1.loc[test_index_class].values, p_y_given_x2.loc[test_index_class].values,\n",
    "                                            p_y_given_x1_x2.loc[test_index_class].values,\n",
    "                                            label_df[classification_setting].loc[test_index_class].values,\n",
    "                                            ipw_weights.loc[test_index_class].values)}\n",
    "\n",
    "    pickle.dump(all_setting_decompositions, open(path_decomposition, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming = {\n",
    "    'Unique 1': 'unique1', \n",
    "    'Unique 2': 'unique2', \n",
    "    'Shared': 'shared', \n",
    "    'Complementary': 'complementary',\n",
    "    r'$X_1$': 'x2',\n",
    "    r'$X_2$': 'x1',\n",
    "    r'$X_1 + X_2$': 'x1_x2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 'observed' # 'corrected' or 'observed'\n",
    "evaluation = 'observed' # 'all', 'observed', 'corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_parent_path = os.path.join(PLOT_PATH, 'plots')\n",
    "create_path_and_all_parents(current_parent_path)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Estimated PID')\n",
    "plt.ylabel('Oracle PID')\n",
    "for pid, color in zip(['Unique 1', 'Unique 2', 'Shared', 'Complementary'], ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']):\n",
    "    pids = [all_setting_decompositions[classification_setting][training][evaluation][naming[pid]] for classification_setting in all_setting_decompositions]\n",
    "    oracle_pids = [all_setting_decompositions[classification_setting]['all']['all'][naming[pid]] for classification_setting in all_setting_decompositions]\n",
    "    plt.scatter(pids, oracle_pids, alpha = 0.25, color = color, s = 100)\n",
    "\n",
    "    # Calculate RMSE and fit\n",
    "    perfs, perfs_oracle = np.array(pids), np.array(oracle_pids)\n",
    "    x = np.linspace(0, 0.25, 100)\n",
    "    curves, slopes, rmse = [], [], []\n",
    "    np.random.seed(0)\n",
    "    for i in range(100):\n",
    "        sample = np.random.choice(len(perfs), size = len(perfs), replace = True)\n",
    "        m, c = np.polyfit(perfs[sample], perfs_oracle[sample], 1)\n",
    "        curves.append(m * x + c)\n",
    "        slopes.append(m)\n",
    "        rmse.append(root_mean_squared_error(perfs_oracle[sample], perfs[sample]))\n",
    "\n",
    "    mean = np.mean(curves, axis = 0)\n",
    "    std = np.std(curves, axis = 0)\n",
    "    plt.fill_between(x, mean - std, mean + std, color = color, alpha = 0.25)\n",
    "    plt.plot(x, mean, color = color, ls = '--', alpha = 0.75, lw = 2, label = r'$\\epsilon =$' + '{:.3f} ({:.3f})'.format(np.mean(rmse), np.std(rmse)))\n",
    "    plt.xlim(-0.01, 0.25)\n",
    "    plt.ylim(-0.01, 0.35)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig(os.path.join(current_parent_path, f'{training}_{evaluation}_pid.png'), bbox_inches = 'tight', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_parent_path = os.path.join(PLOT_PATH, 'plots')\n",
    "create_path_and_all_parents(current_parent_path)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Estimated AUC')\n",
    "plt.ylabel('Oracle AUC')\n",
    "for model, color in zip([r'$X_1$', r'$X_2$', r'$X_1 + X_2$'], ['#1f77b4', '#ff7f0e', '#2ca02c']):\n",
    "    perfs = [all_setting_metrics[classification_setting][naming[model]][training][evaluation]['auroc'] for classification_setting in all_setting_metrics]\n",
    "    perfs_oracle = [all_setting_metrics[classification_setting][naming[model]]['all']['all']['auroc'] for classification_setting in all_setting_metrics]\n",
    "\n",
    "    plt.scatter(perfs, perfs_oracle, alpha = 0.25, color = color, s = 100)\n",
    "\n",
    "    perfs, perfs_oracle = np.array(perfs), np.array(perfs_oracle)\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    curves, slopes, rmse = [], [], []\n",
    "    np.random.seed(0)\n",
    "    for i in range(100):\n",
    "        sample = np.random.choice(len(perfs), size = len(perfs), replace = True)\n",
    "        m, c = np.polyfit(perfs[sample], perfs_oracle[sample], 1)\n",
    "        curves.append(m * x + c)\n",
    "        slopes.append(m)\n",
    "        rmse.append(root_mean_squared_error(perfs_oracle[sample], perfs[sample]))\n",
    "\n",
    "    mean = np.mean(curves, axis = 0)\n",
    "    std = np.std(curves, axis = 0)\n",
    "    plt.fill_between(x, mean - std, mean + std, color = color, alpha = 0.25)\n",
    "    plt.plot(x, mean, color = color, ls = '--', alpha = 0.75, lw = 2, label = r'$\\epsilon =$' + '{:.3f} ({:.3f})'.format(np.mean(rmse), np.std(rmse)))\n",
    "    plt.xlim(0.5, 0.8)\n",
    "    plt.ylim(0.5, 0.8)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(current_parent_path, f'{training}_{evaluation}_perf.png'), bbox_inches = 'tight', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize correlation PID and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = 'all' # 'all', 'observed', 'corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_parent_path = os.path.join(PLOT_PATH, 'plots')\n",
    "create_path_and_all_parents(current_parent_path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, sharey=True, figsize=(14, 5))\n",
    "axes[0].set_ylabel('Oracle AUC')\n",
    "for pid, ax in zip(['Unique 1', 'Unique 2', 'Shared', 'Complementary'], axes):\n",
    "    ax.set_xlabel(pid)\n",
    "    for model, color in zip([r'$X_1$', r'$X_2$', r'$X_1 + X_2$'], ['#1f77b4', '#ff7f0e', '#2ca02c']):\n",
    "        pids = [all_setting_decompositions[classification_setting][evaluation][evaluation][naming[pid]] for classification_setting in all_setting_metrics]\n",
    "        perfs_oracle = [all_setting_metrics[classification_setting][naming[model]]['all']['all']['auroc'] for classification_setting in all_setting_metrics]\n",
    "\n",
    "        ax.scatter(pids, perfs_oracle, alpha = 0.25, color = color, s = 100)\n",
    "\n",
    "        pids, perfs_oracle = np.array(pids), np.array(perfs_oracle)\n",
    "        x = np.linspace(0, pids.max(), 100)\n",
    "        curves, slopes, corr = [], [], []\n",
    "        np.random.seed(0)\n",
    "        for i in range(100):\n",
    "            sample = np.random.choice(len(pids), size = len(pids), replace = True)\n",
    "            m, c = np.polyfit(pids[sample], perfs_oracle[sample], 1)\n",
    "            curves.append(m * x + c)\n",
    "            slopes.append(m)\n",
    "            corr.append(np.corrcoef(pids[sample], perfs_oracle[sample])[0, 1])\n",
    "\n",
    "        mean = np.mean(curves, axis = 0)\n",
    "        std = np.std(curves, axis = 0)\n",
    "        ax.fill_between(x, mean - std, mean + std, color = color, alpha = 0.25)\n",
    "        ax.plot(x, mean, color = color, ls = '--', alpha = 0.75, lw = 2, label = r'$\\alpha =$' + '{:.2f}'.format(np.mean(slopes)))\n",
    "\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.legend(loc='upper center',  bbox_to_anchor=(0.5, 1.))\n",
    "if evaluation == 'observed':\n",
    "    plt.suptitle('Current Practice')\n",
    "elif evaluation == 'corrected':\n",
    "    plt.suptitle(r'$ICYM^2I$')\n",
    "else:\n",
    "    plt.suptitle('Oracle')\n",
    "plt.savefig(os.path.join(current_parent_path, f'{evaluation}_connection.png'), bbox_inches = 'tight', dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
