{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to reproduce the semi-synethic results on the humour dataset.\n",
    "\n",
    "Please download the dataset from []()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils_generation import *\n",
    "from utils_classification import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False, \"axes.spines.left\": False,\n",
    "                 \"axes.spines.bottom\": False, \"figure.dpi\": 300, 'savefig.dpi': 300}\n",
    "sns.set_theme(style = \"whitegrid\", rc = custom_params, font_scale = 1.75)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/multimodal/UR-FUNNY_preprocessed/'\n",
    "RESULTS_PATH = './results_humour'\n",
    "\n",
    "PLOT_PATH = os.path.join(RESULTS_PATH, 'plots')\n",
    "PRED_PATH = os.path.join(RESULTS_PATH, 'preds')\n",
    "\n",
    "\n",
    "# Define path to open data\n",
    "create_path_and_all_parents(RESULTS_PATH)\n",
    "create_path_and_all_parents(PLOT_PATH)\n",
    "create_path_and_all_parents(PRED_PATH)\n",
    "\n",
    "print(f'Saving results to {RESULTS_PATH}')\n",
    "\n",
    "data_path_dict = {'x1_feature_path':os.path.join(DATA_PATH, 'audio.csv'),\n",
    "                  'x2_feature_path':os.path.join(DATA_PATH, 'vision.csv'),\n",
    "                  'x3_feature_path':os.path.join(DATA_PATH, 'text.csv'),\n",
    "                  'classification_label_path':os.path.join(DATA_PATH, 'labels.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV open\n",
    "x1_df = pd.read_csv(data_path_dict['x3_feature_path'], index_col=0) # text\n",
    "x2_df = pd.concat([pd.read_csv(data_path_dict['x1_feature_path'], index_col=0), pd.read_csv(data_path_dict['x2_feature_path'], index_col=0)], axis = 1).dropna() # video\n",
    "\n",
    "# Standardize\n",
    "x1_df = pd.DataFrame(StandardScaler().fit_transform(x1_df), \n",
    "                     columns=x1_df.columns, index=x1_df.index)\n",
    "x2_df = pd.DataFrame(StandardScaler().fit_transform(x2_df), \n",
    "                     columns=x2_df.columns, index=x2_df.index)\n",
    "\n",
    "x1_x2_df = pd.concat([x1_df, x2_df], axis = 1)\n",
    "label_df = pd.read_csv(data_path_dict['classification_label_path'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic missingness - text is not observed for everyone (missingness)\n",
    "p_m_1_array = np.arange(0.3, 0.8, 0.1).round(3)\n",
    "\n",
    "\n",
    "missingness_prob = generate_missingness(x1_df, x2_df, label_df.classification_label, p_m_1_array)\n",
    "missingness_label = missingness_prob.apply(lambda x: np.random.binomial(1, x, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = label_df.loc[label_df.loc[:, 'data_split'] == 'train'].index\n",
    "val_index = label_df.loc[label_df.loc[:, 'data_split'] == 'valid'].index\n",
    "test_index = label_df.loc[label_df.loc[:, 'data_split'] == 'test'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'layers': [[32] * 2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_predictions = os.path.join(PRED_PATH, 'all_setting_predictions.pickle')\n",
    "path_metrics = os.path.join(PRED_PATH, 'all_setting_metrics.pickle')\n",
    "\n",
    "if os.path.isfile(path_predictions):\n",
    "    all_setting_predictions = pickle.load(open(path_predictions, 'rb'))\n",
    "    all_setting_metrics = pickle.load(open(path_metrics, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_predictions = {}\n",
    "    all_setting_metrics = {}\n",
    "\n",
    "for observation in ['all', 'observed', 'corrected'] :\n",
    "    for modality, modality_name in zip([x1_df, x2_df, x1_x2_df], ['x1', 'x2', 'x1_x2']):    \n",
    "        if (modality_name, observation) in all_setting_predictions: continue    \n",
    "\n",
    "        all_setting_predictions[(modality_name, observation)] = {}\n",
    "        all_setting_metrics[(modality_name, observation)] = {}\n",
    "\n",
    "        if observation == 'all':\n",
    "            # Train with normalization\n",
    "            all_setting_predictions[(modality_name, observation)] = train_mlp_and_get_prediction_probabilities(modality.loc[train_index], label_df.classification_label.loc[train_index], \n",
    "                                                                                                                modality.loc[val_index], label_df.classification_label.loc[val_index], \n",
    "                                                                                                                modality, grid_search=grid_search)\n",
    "\n",
    "            # Evaluate    \n",
    "            all_setting_metrics[(modality_name, observation)] = get_classification_metric_dict(y_true= label_df.classification_label.loc[test_index], \n",
    "                                                                                                        y_pred = all_setting_predictions[(modality_name, observation)].loc[test_index])\n",
    "        \n",
    "        else:\n",
    "            for p_m_1 in p_m_1_array:\n",
    "                # Compute under missingness \n",
    "                observed = missingness_label.loc[:, p_m_1] == 0\n",
    "                data = modality.loc[observed]\n",
    "\n",
    "                # Split data\n",
    "                train = data.loc[label_df.loc[observed, 'data_split'] == 'train']\n",
    "                val = data.loc[label_df.loc[observed, 'data_split'] == 'valid']\n",
    "                test = data.loc[label_df.loc[observed, 'data_split'] == 'test']\n",
    "                eval = test\n",
    "                \n",
    "\n",
    "                # Estimate IPW weights\n",
    "                p_m = missingness_label.loc[:, p_m_1].mean() # observed\n",
    "                p_hat = missingness_prob.loc[:, p_m_1] # estimated\n",
    "                ipw_weights = (1-p_m) / (1-p_hat)  \n",
    "\n",
    "                # Train with IPW\n",
    "                all_setting_predictions[(modality_name, observation)][p_m_1] = train_mlp_and_get_prediction_probabilities(train, label_df.classification_label.loc[train.index], \n",
    "                                                                                                                        val, label_df.classification_label.loc[val.index], \n",
    "                                                                                                                        modality, \n",
    "                                                                                                                        sample_weight=ipw_weights.loc[train.index] if observation == 'corrected' else None, \n",
    "                                                                                                                        weight_val=ipw_weights.loc[val.index] if observation == 'corrected' else None, \n",
    "                                                                                                                        grid_search=grid_search)\n",
    "\n",
    "                # Evaluate\n",
    "                all_setting_metrics[(modality_name, observation)][p_m_1] = get_classification_metric_dict(y_true= label_df.classification_label.loc[eval.index], \n",
    "                                                                                                            y_pred = all_setting_predictions[(modality_name, observation)][p_m_1].loc[eval.index],\n",
    "                                                                                                            ipw_weights= ipw_weights.loc[eval.index] if observation == 'corrected' else None)\n",
    "                    \n",
    "        pickle.dump(all_setting_predictions, open(path_predictions, 'wb'))\n",
    "        pickle.dump(all_setting_metrics, open(path_metrics, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.02\n",
    "unadjusted_marker = 'x'\n",
    "adjusted_marker = 'o'\n",
    "all_marker = 'D'\n",
    "alpha_value = 0.5\n",
    "\n",
    "x_color =  ['#648fff', '#dc267f', '#fe6100']\n",
    "x_offset = [-width, 0, width]\n",
    "\n",
    "clean = {\n",
    "    'causal_shared': 'Shared', \n",
    "    'causal_unique_1': 'Unique 1', \n",
    "    'causal_unique_2': 'Unique 2', \n",
    "    'causal_complementary': 'Complementary'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_parent_path = os.path.join(PLOT_PATH, 'ipw_plots')\n",
    "create_path_and_all_parents(ipw_parent_path)\n",
    "\n",
    "for current_metric in ['auroc'] :\n",
    "    # Naming\n",
    "    if current_metric == 'bce_loss' :\n",
    "        metric_name_for_title = 'Binary Cross Entropy'\n",
    "    else :\n",
    "        metric_name_for_title = current_metric.upper()\n",
    "\n",
    "    \n",
    "    super_title = f'{metric_name_for_title}'\n",
    "\n",
    "    fig, ax = plt.subplots(2,1, figsize = (12, 7))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, p_m_1 in enumerate(p_m_1_array) :      \n",
    "        p_m_1_float = float(p_m_1)\n",
    "        for j, (modality, name) in enumerate(zip(['x1', 'x2', 'x1_x2'], ['$X_1$', '$X_2$', 'Multimodal'])):\n",
    "            unadjusted = all_setting_metrics[(modality, 'observed')][p_m_1][current_metric]\n",
    "            adjusted = all_setting_metrics[(modality, 'corrected')][p_m_1][current_metric]\n",
    "            all = all_setting_metrics[(modality, 'all')][current_metric]\n",
    "\n",
    "            unadjusted_err = all_setting_metrics[(modality, 'observed')][p_m_1][current_metric + '_std']\n",
    "            adjusted_err = all_setting_metrics[(modality, 'corrected')][p_m_1][current_metric + '_std']\n",
    "            all_err = all_setting_metrics[(modality, 'all')][current_metric + '_std']\n",
    "\n",
    "            ax[0].scatter(p_m_1_float + x_offset[j], unadjusted, color=x_color[j], marker = unadjusted_marker, linewidths = 2, label = name + ' Unadjusted' if (i == 0) else None, alpha = alpha_value, s = 100)\n",
    "            ax[0].errorbar(p_m_1_float + x_offset[j], unadjusted, yerr=unadjusted_err, color=x_color[j], label = None)\n",
    "            ax[0].scatter(p_m_1_float + x_offset[j], adjusted, color=x_color[j], marker = adjusted_marker, label = name + ' Adjusted' if (i == 0) else None, alpha = alpha_value, s = 100)\n",
    "            ax[0].errorbar(p_m_1_float + x_offset[j], adjusted, yerr=adjusted_err, color=x_color[j], label = None)\n",
    "            ax[0].scatter(p_m_1_float + x_offset[j], all, color=x_color[j], marker = all_marker, label = name + ' Oracle' if (i == 0) else None, alpha = alpha_value, s = 100)\n",
    "            ax[0].errorbar(p_m_1_float + x_offset[j], all, yerr= all_err, color=x_color[j], label = None)\n",
    "\n",
    "            ax[1].scatter(p_m_1_float + x_offset[j], np.abs(unadjusted - all) - np.abs(adjusted - all), color=x_color[j], label = None, alpha = alpha_value, s = 100)\n",
    "\n",
    "        if i < len(p_m_1_array) - 1:\n",
    "            ax[0].axvline(p_m_1_float + 0.05, color = 'grey', alpha = 0.25, linestyle = 'dotted')\n",
    "            ax[1].axvline(p_m_1_float + 0.05, color = 'grey', alpha = 0.25, linestyle = 'dotted')\n",
    "\n",
    "    ax[1].axhline(0, color = 'grey', alpha = 0.25, linestyle = 'dotted')\n",
    "    ax[0].set_xticklabels([])\n",
    "    ax[1].set_xlabel('Missingness Rate - $P(m_2=1)$', fontweight = 'bold')\n",
    "    ax[0].legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "    ax[0].set_ylabel(metric_name_for_title)\n",
    "    ax[1].set_ylabel('Absolute error difference')\n",
    "\n",
    "    ax[0].grid(axis='x', visible=False)\n",
    "    ax[1].grid(axis='x', visible=False)\n",
    "\n",
    "    if current_metric == 'auroc' :\n",
    "        ax[0].set_ylim([.4, .8])\n",
    " \n",
    "\n",
    "    fig.suptitle(super_title, fontweight = 'bold')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    current_parent_path = os.path.join(ipw_parent_path, current_metric)\n",
    "    create_path_and_all_parents(current_parent_path)\n",
    "    plt.subplots_adjust(hspace=0.1)\n",
    "    plt.savefig(os.path.join(current_parent_path, f'{super_title}.png'), bbox_inches = 'tight', dpi = 400)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from information_decomposition import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_decomposition = os.path.join(PRED_PATH, 'all_setting_decompositions.pickle')\n",
    "\n",
    "if os.path.isfile(path_decomposition):\n",
    "    all_setting_decompositions = pickle.load(open(path_decomposition, 'rb'))\n",
    "else:\n",
    "    # Predictions across info settings\n",
    "    all_setting_decompositions = {}\n",
    "\n",
    "if 'all' not in all_setting_decompositions:\n",
    "    p_y_given_x1_x2 = all_setting_predictions[('x1_x2', 'all')]\n",
    "    p_y_given_x1 = all_setting_predictions[('x1', 'all')]\n",
    "    p_y_given_x2 = all_setting_predictions[('x2', 'all')]\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train_index].values, x2_df.loc[train_index].values,\n",
    "                                x1_df.loc[val_index].values, x2_df.loc[val_index].values,\n",
    "                                p_y_given_x1.loc[train_index].values, p_y_given_x2.loc[train_index].values, \n",
    "                                p_y_given_x1.loc[val_index].values, p_y_given_x2.loc[val_index].values,\n",
    "                                grid_search=grid_search, epochs = 100)\n",
    "\n",
    "    all_setting_decompositions = {\n",
    "        'all': pid_decomposition_batched(estimator, x1_df.loc[test_index].values, x2_df.loc[test_index].values, \n",
    "                                        p_y_given_x1.loc[test_index].values, p_y_given_x2.loc[test_index].values, \n",
    "                                        p_y_given_x1_x2.loc[test_index].values, \n",
    "                                        label_df.classification_label.loc[test_index].values),\n",
    "    }\n",
    "    pickle.dump(all_setting_decompositions, open(path_decomposition, 'wb'))\n",
    "    print('all', all_setting_decompositions['all'])\n",
    "\n",
    "for p_m_1 in tqdm(p_m_1_array):\n",
    "    if ('observed', p_m_1) in all_setting_decompositions: continue\n",
    "    \n",
    "    # Compute under missingness \n",
    "    observed = missingness_label.loc[:, p_m_1] == 0\n",
    "\n",
    "    # Split data\n",
    "    train = label_df.loc[observed, 'data_split'] == 'train'\n",
    "    val = label_df.loc[observed, 'data_split'] == 'valid'\n",
    "    test = label_df.loc[observed, 'data_split'] == 'test'\n",
    "    train, val, test = train[train], val[val], test[test]\n",
    "\n",
    "    p_m = missingness_label.loc[:, p_m_1].mean() # observed\n",
    "    p_hat = missingness_prob.loc[:, p_m_1]\n",
    "    ipw_weights = (1 - p_m) / (1 - p_hat)  \n",
    "\n",
    "    # Estimate with IPW weights\n",
    "    p_y_given_x1_x2 = all_setting_predictions[('x1_x2', 'corrected')][p_m_1]\n",
    "    p_y_given_x1 = all_setting_predictions[('x1', 'corrected')][p_m_1]\n",
    "    p_y_given_x2 = all_setting_predictions[('x2', 'corrected')][p_m_1]\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train.index].values, x2_df.loc[train.index].values, \n",
    "                                x1_df.loc[val.index].values, x2_df.loc[val.index].values, \n",
    "                                p_y_given_x1.loc[train.index].values, p_y_given_x2.loc[train.index].values, \n",
    "                                p_y_given_x1.loc[val.index].values, p_y_given_x2.loc[val.index].values,\n",
    "                                ipw_weights.loc[train.index].values, ipw_weights.loc[val.index].values,\n",
    "                                grid_search=grid_search, epochs = 100)\n",
    "\n",
    "    all_setting_decompositions[('corrected', p_m_1)] = pid_decomposition_batched(estimator, x1_df.loc[test.index].values, x2_df.loc[test.index].values, \n",
    "                                                                                p_y_given_x1.loc[test.index].values, p_y_given_x2.loc[test.index].values,\n",
    "                                                                                p_y_given_x1_x2.loc[test.index].values, \n",
    "                                                                                label_df.classification_label.loc[test.index].values, \n",
    "                                                                                ipw_weights.loc[test.index].values)\n",
    "\n",
    "    # Compute with no correction\n",
    "    p_y_given_x1_x2 = all_setting_predictions[('x1_x2', 'observed')][p_m_1]\n",
    "    p_y_given_x1 = all_setting_predictions[('x1', 'observed')][p_m_1]\n",
    "    p_y_given_x2 = all_setting_predictions[('x2', 'observed')][p_m_1]\n",
    "\n",
    "    estimator = QEstimator(x1_df.loc[train.index].values, x2_df.loc[train.index].values,\n",
    "                    x1_df.loc[val.index].values, x2_df.loc[val.index].values,\n",
    "                    p_y_given_x1.loc[train.index].values, p_y_given_x2.loc[train.index].values, \n",
    "                    p_y_given_x1.loc[val.index].values, p_y_given_x2.loc[val.index].values,\n",
    "                    grid_search=grid_search, epochs = 100, device = 'cuda:1')\n",
    "\n",
    "    all_setting_decompositions[('observed', p_m_1)] = pid_decomposition_batched(estimator, x1_df.loc[test.index].values, x2_df.loc[test.index].values, \n",
    "                                                                                p_y_given_x1.loc[test.index].values, p_y_given_x2.loc[test.index].values, \n",
    "                                                                                p_y_given_x1_x2.loc[test.index].values, \n",
    "                                                                                label_df.classification_label.loc[test.index].values)\n",
    "    pickle.dump(all_setting_decompositions, open(path_decomposition, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_parent_path = os.path.join(PLOT_PATH, 'pid_plots10')\n",
    "create_path_and_all_parents(pid_parent_path)\n",
    "\n",
    "width = 0.015\n",
    "x_color =  ['#648fff', '#dc267f', '#fe6100', '#50C878']\n",
    "x_offset = [-2*width, -width, width, 2*width]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "        \n",
    "ax.scatter([], [], color='grey', marker = unadjusted_marker, linewidths = 2, label = 'Observed', s = 100)\n",
    "ax.scatter([], [], color='grey', marker = adjusted_marker, linewidths = 2, label = r'ICYM$^2$I', s = 100)\n",
    "ax.scatter([], [], color='grey', marker = all_marker, linewidths = 2, label = 'Oracle', s = 100)\n",
    "ax.scatter([], [], alpha = 0, label = ' ')\n",
    "\n",
    "for i, p_m_1 in enumerate(p_m_1_array) :      \n",
    "    p_m_1_float = float(p_m_1)\n",
    "\n",
    "    for j, decomposition in enumerate(['unique1', 'unique2', 'complementary', 'shared']):\n",
    "        unadjusted = max(all_setting_decompositions[('observed', p_m_1)][decomposition], 0.)\n",
    "        adjusted = max(all_setting_decompositions[('corrected', p_m_1)][decomposition], 0.)\n",
    "        all_res = max(all_setting_decompositions['all'][decomposition], 0)\n",
    "\n",
    "        unadjusted_err = all_setting_decompositions[('observed', p_m_1)][decomposition + '_std']\n",
    "        adjusted_err = all_setting_decompositions[('corrected', p_m_1)][decomposition + '_std']\n",
    "        all_err = all_setting_decompositions['all'][decomposition + '_std']\n",
    "\n",
    "        ax.scatter(p_m_1_float + x_offset[j], unadjusted, color=x_color[j], marker = unadjusted_marker, linewidths = 2, alpha = alpha_value, s = 200)           \n",
    "        ax.errorbar(p_m_1_float + x_offset[j], unadjusted, yerr=unadjusted_err, color=x_color[j])\n",
    "        ax.scatter(p_m_1_float + x_offset[j], adjusted, color=x_color[j], marker = adjusted_marker,  alpha = alpha_value, s = 200)\n",
    "        ax.errorbar(p_m_1_float + x_offset[j], adjusted, yerr=adjusted_err, color=x_color[j])\n",
    "        ax.scatter(p_m_1_float + x_offset[j], all_res, color=x_color[j], marker = all_marker, alpha = alpha_value, s = 200)\n",
    "        ax.errorbar(p_m_1_float + x_offset[j], all_res, yerr= all_err, color=x_color[j])\n",
    "\n",
    "        if i == 0:\n",
    "            ax.scatter([], [], color=x_color[j], marker = 's', label = decomposition.capitalize(), s = 200)\n",
    "\n",
    "    if i < len(p_m_1_array) - 1:\n",
    "        ax.axvline(p_m_1_float + 0.05, color = 'grey', alpha = 0.25, linestyle = 'dotted')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.grid(axis='x', visible=False)\n",
    "\n",
    "plt.ylabel('PID Values')\n",
    "plt.xlabel('Missigness Rate')\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))\n",
    "\n",
    "plt.savefig(os.path.join(pid_parent_path, f'{super_title}.png'), bbox_inches = 'tight', dpi = 400)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
